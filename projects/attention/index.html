<!DOCTYPE html>
<html lang="en">

<!-- Begin Jekyll SEO tag v2.8.0 -->
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>Visualizing Attention | Cynthia Chen</title>
<meta name="generator" content="Jekyll v4.3.2">
<meta property="og:title" content="Visualizing Attention">
<meta property="og:locale" content="en">
<meta name="description" content="Studying transformer attention via experiments with query and key embeddings.">
<meta property="og:description" content="Studying transformer attention via experiments with query and key embeddings.">
<link rel="canonical" href="https://chenxcynthia.github.io/projects/attention/">
<meta property="og:url" content="https://chenxcynthia.github.io/projects/attention/">
<meta property="og:site_name" content="Cynthia Chen">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2023-10-19T17:22:58+00:00">
<meta name="twitter:card" content="summary">
<meta property="twitter:title" content="Visualizing Attention">
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2023-10-19T17:22:58+00:00","datePublished":"2023-10-19T17:22:58+00:00","description":"Studying transformer attention via experiments with query and key embeddings.","headline":"Visualizing Attention","mainEntityOfPage":{"@type":"WebPage","@id":"https://chenxcynthia.github.io/projects/attention/"},"url":"https://chenxcynthia.github.io/projects/attention/"}</script>
<!-- End Jekyll SEO tag -->


  <!-- Head -->
      <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Visualizing Attention | Cynthia Chen</title>
    <meta name="author" content="Cynthia  Chen">
    <meta name="description" content="Studying transformer attention via experiments with query and key embeddings.">
    <meta name="keywords" content="cynthia, chen">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    <!-- Styles -->
    
    <link rel="shortcut icon" href="/assets/img/icon.png">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://chenxcynthia.github.io/projects/attention/">
    
    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark">

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    

  

  <!-- Body -->
  </head>
<body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/">Cynthia Chen</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">about</a>
              </li>
              

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/projects/">projects</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">publications</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/cv/">cv</a>
              </li>
              <li class="nav-item dropdown ">
                <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">more</a>
                <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown">
                  <a class="dropdown-item" href="/art/">art</a>
                </div>
              </li>

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="container mt-5">
      <!-- page.html -->
<div class="post">

  <header class="post-header">
    <h1 class="post-title">Visualizing Attention</h1>
    <p class="post-description">Studying transformer attention via experiments with query and key embeddings.</p>
  </header>

  <article>
    <div class="projheader">
    <div class="links"><a href="https://arxiv.org/abs/2305.03210" class="btn z-depth-0" role="button" rel="external nofollow noopener" target="_blank"> Paper </a></div>
    <div class="links"><a href="https://github.com/chenxcynthia/transformer-interpretability/tree/main/Attention%20Visualization" class="btn z-depth-0" role="button" rel="external nofollow noopener" target="_blank"> <i class="fab fa-github gh-icon"></i> Github</a></div>
</div>

<p><strong>AttentionViz.</strong> The self-attention mechanism in transformer models plays a critical role in helping the model learn a rich set of relationships between input elements. To assist in our understanding of attention, <a href="https://arxiv.org/abs/2305.03210" rel="external nofollow noopener" target="_blank">Yeh et al.</a> developed <a href="http://attentionviz.com/" rel="external nofollow noopener" target="_blank">AttentionViz</a>, a tool that enables the visualization of attention patterns at a more global scale. In particular, AttentionViz introduces a technique for jointly visualizing query and key vectors—two of the core components in computing attention—in a shared embedding space. In AttentionViz, every query and key (originally a 64-dimensional vector) is projected to a 2-dimensional embedding space using t-SNE or UMAP. Queries and keys are jointly displayed on the same plot, allowing for the visualization of distinct attention patterns among queries and keys.</p>

<p><strong>Distance as a proxy for attention.</strong> A critical idea here is that in the AttentionViz visualizations, we
want distance to be an accurate proxy for attention: high-attention query-key pairs should be closer together in the joint embeddings, a relationship depicted in Figure 1b. To optimize for this desired distance-attention
relationship, we can take a look at how attention is computed based on the q (query), k (key), and v (value)
vectors:</p>

\[\texttt{attention}(q, k, v) = \textrm{softmax}(\frac{qk^T}{\sqrt{d_k}})v\]

<p>We see that attention directly corresponds to the dot product between the query and key vector. Therefore, if we are aiming for <em>small distance</em> to be a proxy for <em>high attention</em>, then we want the query-key dot product and distance to have a strong, inverse correlation. Put mathematically, we want the correlation between \(\texttt{dot-product}(q, k)\) and \(\texttt{distance}(q, k)\) to be as close to -1 as possible.</p>

<p><strong>Optimizing correlation.</strong> How can we optimize the correlation between the dot product and distance between queries and keys without losing the integrity of the attention computation? Luckily, there are two ``free parameters’’ when computing attention: translation and scaling. The operations of <em>translation</em> (shifting query and key vectors by a constant vector) and <em>scaling in opposite directions</em> (multiplying query vectors by \(c\) and dividing key vectors by \(c\)) can both be performed without changing the resulting attention value. In the following experiments, we largely focus on scaling and identifying the scaling constant \(c\) that provides the best correlation between dot product and distance.</p>

<p>To determine the optimal value of \(c\), we can define a *weighted correlation *metric that places heavier weight on query-key pairs with smaller distances, since we care most about nearby queries and keys in the joint visualization. We first computed a distance threshold \(d\), defined as the 0.5 percentile value of the distance distribution within a specific attention head. For every query-key pair with distance $d_i &lt; d$, we compute the weighted correlation as follows:</p>

\[\texttt{weighted-corr}(x, y, w) = \frac{\textrm{cov}(x, y; w)}{\sqrt{\textrm{cov}(x, x; w) \textrm{cov}(y, y; w)}}\]

<p>The weights \(w\) are defined as \((d - d_i)^2\) which assigns more weight to query-key pairs that are closer to one another. We then choose the value of \(c\) that gives a weighted correlation closest to -1.</p>

<p>Building off of the weighted correlation metric, we defined a second optimization metric (<em>weighted correlation, scaled</em>) as follows. Within each scaling factor, we also kept a count of the number of instances of key-query pairs with distance less than the distance threshold. We then enumerated the number of instances across all the attention heads and normalized all weighted correlations within the scaling factor by this count. Again, we choose a value of \(c\) that brings this scaled weighted correlation value closest to -1.</p>

<p>A final metric that we experimented with is the <em>ratio of the median query norm to the median key norm</em>. Differences in norm can cause distance and dot product to diverge from one another; as such, we reasoned that standardizing the query and key norms would bring the correlation closer. Rather than maximizing the correlation here, we simply set \(c\) to be the square root of the ratio itself, as scaling by \(c\) will automatically standardize the query and key norms.</p>

<p>For each attention head, we can thus choose the scale factors \(c\) that optimize the three metrics described above. For each of the metrics, we ran experiments with constants  \(c \in [0.2, 0.4, 0.8, 1, 1.25, 2.5, 5]\). Future work could explore the results of a greater range and granularity of constant values. The optimal scaling constants for each metric are displayed in the heatmaps in Figure 1 below.</p>

<div class="row justify-content-sm-center">
    <div class="col-sm-10 mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/proj/attn/optimal_constants-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/proj/attn/optimal_constants-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/proj/attn/optimal_constants-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/proj/attn/optimal_constants.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Figure 1: The optimal scaling constants for each attention head, as computed under the three defined metrics—(a) weighted correlation, (b) weighted correlation, scaled, and (c) ratio of Q/K norms—are displayed as heatmaps.
</div>

<p><strong>Scaling queries and keys.</strong> Here, we show examples of the resulting embedding visualization of keys and queries after they have been scaled. In Figures 2 and 3, we display the joint embeddings for six constants and highlight the visualization with the optimal scaling constant identified by the <em>weighted correlation (scaled)</em> metric as shown in Figure 1b.</p>

<div class="row justify-content-sm-center">
    <div class="col-sm-10 mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/proj/attn/L1H2-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/proj/attn/L1H2-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/proj/attn/L1H2-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/proj/attn/L1H2.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Figure 2: The optimal scaling constants for each attention head, as computed under the three defined metrics—(a) weighted correlation, (b) weighted correlation, scaled, and (c) ratio of Q/K norms—are displayed as heatmaps.
</div>

<div class="row justify-content-sm-center">
    <div class="col-sm-10 mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/proj/attn/L3H3-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/proj/attn/L3H3-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/proj/attn/L3H3-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/proj/attn/L3H3.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Figure 3: The optimal scaling constants for each attention head, as computed under the three defined metrics—(a) weighted correlation, (b) weighted correlation, scaled, and (c) ratio of Q/K norms—are displayed as heatmaps.
</div>

<p>For both of the cases displayed, our method chooses a value of \(c\) that yields a strong visualization where the query and key vector clouds are overlapping rather than disjoint. Note that these are visualizations of the query and key embeddings after they have been scaled by the respective constant and then translated so the query and key clouds have the same centroid. Embeddings are generated  UMAP using the cosine distance metric.</p>

<p><strong>Future directions.</strong>  There are several directions in which this work could be continued or extended. First, though we focus on optimizing the correlation between dot product and distance, it remains unclear whether this is the best proxy for attention visualization quality. There may well be several other metrics that could be employed (including the ratio of norms, like we explore in the third metric). Furthermore, the current visualizations only show the query and key embeddings and attention patterns at large and do not depict any particular relationships between individual queries and keys. Future work could look into investigating certain patterns in the visualizations at a more zoomed-in level (e.g: Do noun queries attend to pronoun keys? For a given attention head, how does it match keys and queries?).</p>

<p> </p>

<p>This project was conducted at the <a href="https://insight.seas.harvard.edu/" rel="external nofollow noopener" target="_blank">Insight + Interaction Lab</a> at Harvard University under the mentorship of <a href="https://catherinesyeh.github.io/" rel="external nofollow noopener" target="_blank">Catherine Yeh</a>, Professor <a href="https://www.bewitched.com/" rel="external nofollow noopener" target="_blank">Martin Wattenberg</a>, and Professor <a href="http://www.fernandaviegas.com/" rel="external nofollow noopener" target="_blank">Fernanda Viégas</a>.</p>

<p><i><strong>Questions or feedback on this project?</strong> Email me at cynthiachen@college.harvard.edu.</i></p>

<p> </p>


  </article>

</div>

    </div>

    <!-- Footer -->    <!-- 
    <footer class="fixed-bottom">
      <div class="container mt-0">
        &copy; Copyright 2023 Cynthia  Chen. 
      </div>
    </footer> -->

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js"></script><!-- Load Common JS -->
  <script defer src="/assets/js/common.js"></script>
  <script defer src="/assets/js/copy_code.js" type="text/javascript"></script>

    

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

  </body>
</html>
