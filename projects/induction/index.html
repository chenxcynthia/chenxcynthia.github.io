<!DOCTYPE html>
<html lang="en">

<!-- Begin Jekyll SEO tag v2.8.0 -->
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>Exploring Induction Heads | Cynthia Chen</title>
<meta name="generator" content="Jekyll v4.3.3">
<meta property="og:title" content="Exploring Induction Heads">
<meta property="og:locale" content="en">
<meta name="description" content="An investigation of in-context learning and induction head behavior in BERT.">
<meta property="og:description" content="An investigation of in-context learning and induction head behavior in BERT.">
<link rel="canonical" href="https://chenxcynthia.github.io/projects/induction/">
<meta property="og:url" content="https://chenxcynthia.github.io/projects/induction/">
<meta property="og:site_name" content="Cynthia Chen">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2024-02-28T17:51:40+00:00">
<meta name="twitter:card" content="summary">
<meta property="twitter:title" content="Exploring Induction Heads">
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2024-02-28T17:51:40+00:00","datePublished":"2024-02-28T17:51:40+00:00","description":"An investigation of in-context learning and induction head behavior in BERT.","headline":"Exploring Induction Heads","mainEntityOfPage":{"@type":"WebPage","@id":"https://chenxcynthia.github.io/projects/induction/"},"url":"https://chenxcynthia.github.io/projects/induction/"}</script>
<!-- End Jekyll SEO tag -->


  <!-- Head -->
      <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Exploring Induction Heads | Cynthia Chen</title>
    <meta name="author" content="Cynthia  Chen">
    <meta name="description" content="An investigation of in-context learning and induction head behavior in BERT.">
    <meta name="keywords" content="cynthia, chen">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    <!-- Styles -->
    
    <link rel="shortcut icon" href="/assets/img/icon.png">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://chenxcynthia.github.io/projects/induction/">
    
    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark">

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    

  

  <!-- Body -->
  </head>
<body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/">Cynthia Chen</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">about</a>
              </li>
              

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/projects/">projects</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">publications</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/cv/">cv</a>
              </li>
              <li class="nav-item dropdown ">
                <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">more</a>
                <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown">
                  <a class="dropdown-item" href="/art/">art</a>
                </div>
              </li>

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="container mt-5">
      <!-- page.html -->
<div class="post">

  <header class="post-header">
    <h1 class="post-title">Exploring Induction Heads</h1>
    <p class="post-description">An investigation of in-context learning and induction head behavior in BERT.</p>
  </header>

  <article>
    <div class="projheader">
    <div class="links"><a href="https://github.com/chenxcynthia/transformer-interpretability/tree/main/Induction%20Heads" class="btn z-depth-0" role="button" rel="external nofollow noopener" target="_blank"> <i class="fab fa-github gh-icon"></i> Github</a></div>
    <div class="links"><a href="https://drive.google.com/file/d/1bQnSFELkAILxAKDd1Z7lZ1eG6qKqss9w/view?usp=share_link" class="btn z-depth-0" role="button" rel="external nofollow noopener" target="_blank"> Paper </a></div>
</div>

<h4 id="motivation">Motivation</h4>
<p><strong>Why study induction heads in BERT?</strong> <em>In-context learning</em> is a phenomenon observed in language models where the models are better at predicting tokens later in the context than earlier ones, even without additional training [1]. In conjunction with observing this phenomenon, previous research has hypothesized that induction heads are the mechanism for the majority of in-context learning [1]. Despite the importance of induction heads, their specific behaviors and why they develop remains a largely unanswered question.</p>

<p>Current literature largely focuses on in-context learning in unidirectional models like GPT. Induction heads have not been previously explored in bidirectional models like BERT; however, the emergent in-context learning behaviors and induction heads found in unidirectional transformer models, as well as cases of prompt-based learning seen in bidirectional models like T5 and BERT [2], point toward the potential existence of induction heads in BERT.</p>

<p>In particular, in this work, we focus here on <em>behavioral</em> induction heads (rather than mechanistic ones), defined as attention heads that exhibit prefix matching or copying behaviors observed through attention patterns on out-of-distribution sequences made of repeated random tokens (RRT) [3].</p>

<p><strong>Confirming in-context learning in BERT.</strong> We first run an initial experiment to check whether BERT exhibits in-context learning. For unidirectional transformer models, Olsson et al. [1] define an in-context learning (ICL) score as the loss of the 500th token in the context minus the loss of the 50th token in the context, averaged over several dataset examples. We replicate a similar heuristic for BERT, where we set the ICL score as the average difference in token-prediction loss for two varying context lengths of 50 and 500:</p>

\[\textrm{ICL Score} = Loss(50 \textrm{-token context}) - Loss(500 \textrm{-token context})\]

<p>We tokenized the Hugging Face \href{https://huggingface.co/datasets/wikipedia}{Wikipedia-simple dataset}, and selected the first 50 and 500 tokens of an article as the two model contexts. We then chose a random token to mask from each context, used BERT to predict the masked token, and computed the loss. Figure 1 displays the ICL scores computed across 50 trials. The mean difference was 0.23, demonstrating a noticeable difference in performance between the two contexts and a signal of in-context learning in BERT.</p>

<div class="row justify-content-sm-center">
    <div class="col-sm-6 mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/proj/induction/icl_loss_difference-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/proj/induction/icl_loss_difference-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/proj/induction/icl_loss_difference-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/proj/induction/icl_loss_difference.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Figure 1: The in-context learning (ICL) score for BERT, defined as the loss difference between a 50-token context and a 500-token context, is shown for 50 trials.
</div>

<h4 id="methods-and-experiment-setups">Methods and Experiment Setups</h4>
<p>To visually explore induction heads in BERT, we drew particular inspiration from the <a href="https://www.neelnanda.io/mosaic" rel="external nofollow noopener" target="_blank">Induction Mosaic</a> [4], a mosaic heatmap of the induction heads in 40 open-source transformer models. In the Induction Mosaic, induction scores were calculated by giving each model a sequence of repeated random tokens and measuring the average attention each head paid to the token after the previous copy of the current token.</p>

<div class="row justify-content-sm-center">
    <div class="col-sm-10 mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/proj/induction/rrt_setups-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/proj/induction/rrt_setups-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/proj/induction/rrt_setups-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/proj/induction/rrt_setups.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Figure 2: The three RRT experiment setups, with arrows denoting the observed attention values placed on ``inductive" tokens by the masked token that were used to compute induction scores. \textbf{(a)} The standard RRT experiment setup, with a series of random tokens followed by the same tokens repeated in the same order. \textbf{(b)} A series of random tokens, followed by the same tokens but in a shuffled order. \textbf{(c)} Bidirectional attention is observed by repeating the same random tokens three times.
</div>

<p>We employ a similar experimental setup for exploring potential induction heads in BERT. First, we generate a sequence of 200 random tokens. We then repeat this sequence of tokens, with several experiment variations (standard, shuffled, repeated on both sides) as shown in Figure 2, to generate a set of random repeated tokens (RRT). We randomly select a token from the second set of random tokens to mask and then pass the masked RRT to the BERT model. From here, we observe the output attention placed from the masked tokens to “inductive” areas, such as the previous instance of the masked token and the tokens before/after the previous instance.</p>

<p>For a specific attention head, the average output attention to these inductive tokens, computed across 50 trials, is defined as the induction score. The induction score represents the likelihood for a specific attention head to be an induction head. We generate an ``induction map” visualization which displays the induction scores for each attention head as a heatmap.</p>

<h4 id="results">Results</h4>

<p><strong>Induction Maps: Standard Setup.</strong> We first generated heatmaps for the standard RRT experiment setup, depicted in Figure 2a, where attention values were observed from the [MASK] token to the previous instance and the tokens before and after the previous instance. The induction maps for these three cases are shown in Figure 3. In the first heatmap, we see that there are several attention heads with strong induction scores. Interestingly, note that all of these high-scoring heads are located in the last three layers. In the cases of observing the attention to the tokens before/after the previous instance, we see in each case that only one head (layer 9 head 9 for the token before and layer 8 head 2 for the token after) gives a strong induction score.</p>

<div class="row justify-content-sm-center">
    <div class="col-sm-12 mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/proj/induction/ind_map_rrt-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/proj/induction/ind_map_rrt-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/proj/induction/ind_map_rrt-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/proj/induction/ind_map_rrt.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Figure 3: Induction map visualizations for the standard RRT experiment setup, depicted in Figure 2a.
</div>

<p><strong>Induction Maps: Variations.</strong> We also generated induction maps for the shuffled and bidirectional RRT experiment setups. For the shuffled version, we observed attention to the token after the previous instance of the token before the masked token (Figure 4a) and the token before the previous instance of the token after the masked token (Figure 4b). Both yield faint induction scores, but interestingly for the same heads as those in Figure 3a. For the bidirectional attention experiment, we observe attention to the previous and later instances of the masked token, shown in Figures 4c and 4d which again yields the same induction patterns as Figure 3a, but attention is now spread across the two instances.</p>

<div class="row justify-content-sm-center">
    <div class="col-sm-9 mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/proj/induction/ind_map_variations-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/proj/induction/ind_map_variations-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/proj/induction/ind_map_variations-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/proj/induction/ind_map_variations.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Figure 4: (a, b) Induction map visualizations for the shuffled RRT experiment setup, depicted in Figure 2b. (c, d) Induction map visualizations for the bidirectional RRT experiment setup, depicted in Figure 2c.
</div>

<p><strong>Do induction heads always appear in later layers?</strong> We observed that all the attention heads with high induction scores occurred in later layers, and were curious to explore this pattern more. To investigate this further, we ran the same RRT experiment on four BERT model variations that have different sizes: Tiny (2 x 2), Small (4 x 8), Medium (8 x 8), Large (24 x 16). The induction maps for each of these models are displayed in Figure 5. For the Small, Medium, and Large BERT models, we notice the same pattern that the high-scoring attention heads all occur in the later layers.</p>

<p>Similar trends of induction heads appearing in later layers are also observed in unidirectional models like GPT, as shown in the Induction Mosaic [4]. A potential hypothesis for why this may be the case is that if induction heads mostly serve to directly predict the next tokens, it makes sense that they’re in later layers as there is less distance to communicate information down.</p>

<div class="row justify-content-sm-center">
    <div class="col-sm-12 mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/proj/induction/bert_sizes-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/proj/induction/bert_sizes-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/proj/induction/bert_sizes-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/proj/induction/bert_sizes.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Figure 5: Induction maps generated for the Tiny, Small, Medium, and Large variations of the BERT model. All of these models present induction heads which only occur in later layers.
</div>

<p><strong>Future directions.</strong>  The results from our initial investigation into induction heads in BERT pose several interesting questions and possible future avenues of exploration. Future work could look into the effects of model ablation, in particular ablating heads with a high induction score (such as L9H9 and L8H2). An interesting question to explore here would be whether a ``backup” induction head would appear if we removed these heads or zeroed out their weights, a behavior which was noted in GPT-2 [5]. Another distinct pattern worth investigating is how induction heads only occur in the latest layers, which could point towards existence of heads performing tasks beyond just direct repetition and copying. Potentially, individual heads could serve unique purposes, such as focusing on language translation, by attending to specific types of tokens. Finally, some other experiments to run would be masking more than one token and observing joint attention, and also exploring the relationship between tokens and words and how attention is distributed amongst them.</p>

<p><strong>References</strong></p>

<p>[1] C. Olsson, N. Elhage, N. Nanda, N. Joseph, N. DasSarma, T. Henighan, B. Mann, A. Askell, Y. Bai,
A. Chen, et al. In-context learning and induction heads. arXiv preprint arXiv:2209.11895, 2022.</p>

<p>[2] A. Patel, B. Li, M. S. Rasooli, N. Constant, C. Raffel, and C. Callison-Burch. Bidirectional language
models are also few-shot learners. arXiv preprint arXiv:2209.14500, 2022.</p>

<p>[3] A. Variengien. Some common confusion about induction heads. https://www.lesswrong.com/posts/
nJqftacoQGKurJ6fv/some-common-confusion-about-induction-heads. Accessed: 2023-04-18.</p>

<p>[4] N. Nanda. Induction Mosaic. https://www.neelnanda.io/mosaic. Accessed: 2023-04-18.</p>

<p>[5] K. Wang, A. Variengien, A. Conmy, B. Shlegeris, and J. Steinhardt. Interpretability in the wild: a circuit
for indirect object identification in GPT-2 small. arXiv preprint arXiv:2211.00593, 2022.</p>

<p> </p>

<p>This project was conducted at the <a href="https://insight.seas.harvard.edu/" rel="external nofollow noopener" target="_blank">Insight + Interaction Lab</a> at Harvard University under the mentorship of <a href="https://catherinesyeh.github.io/" rel="external nofollow noopener" target="_blank">Catherine Yeh</a>, Professor <a href="https://www.bewitched.com/" rel="external nofollow noopener" target="_blank">Martin Wattenberg</a>, and Professor <a href="http://www.fernandaviegas.com/" rel="external nofollow noopener" target="_blank">Fernanda Viégas</a>.</p>

<p><i><strong>Questions or feedback on this project?</strong> Email me at cynthiachen@college.harvard.edu.</i></p>

<p> </p>

  </article>

</div>

    </div>

    <!-- Footer -->    <!-- 
    <footer class="fixed-bottom">
      <div class="container mt-0">
        &copy; Copyright 2024 Cynthia  Chen. 
      </div>
    </footer> -->

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js"></script><!-- Load Common JS -->
  <script defer src="/assets/js/common.js"></script>
  <script defer src="/assets/js/copy_code.js" type="text/javascript"></script>

    

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

  </body>
</html>
